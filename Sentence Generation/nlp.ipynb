{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFR75Q4hTz6h",
        "outputId": "89c2aa27-030f-4640-a42d-2f80b909544e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('words')\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "pos=[]\n",
        "neg=[]\n",
        "total_sen=[]\n",
        "tweets=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLqH57iRe3RA",
        "outputId": "eec0bb54-863d-4ff3-c73e-90076d5c4736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig3.0 is already the newest version (3.0.12-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jamspell in /usr/local/lib/python3.7/dist-packages (0.0.12)\n",
            "File ‘en.tar.gz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y swig3.0\n",
        "!pip install jamspell\n",
        "!wget -nc https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz\n",
        "!tar -xf en.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKFrRYfOe9n8",
        "outputId": "02342bf7-3345-4891-fac5-52837b36a0dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import jamspell\n",
        "corrector = jamspell.TSpellCorrector()\n",
        "corrector.LoadLangModel('en.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMT--7V-lVsG"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import words\n",
        "wordlist=words.words()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g819UXSV7g6"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"A1_dataset.csv\")\n",
        "text=\"\"\n",
        "text_l=[]\n",
        "columns_n = ['LABEL','DATE_TIME','TEXT']\n",
        "processed_dataset = pd.DataFrame(columns=columns_n)\n",
        "c=0\n",
        "\n",
        "for i,d in dataset.iterrows():\n",
        "  c+=1\n",
        "  # print(c)\n",
        "  text = d[\"TEXT\"]\n",
        "  text = text.lower().strip()\n",
        "  text = re.sub(r\"\\<\\w+>\",'',text)\n",
        "  text = re.sub(r\"\\bwww.\\w+.\\w+/?\\w*\",'',text)\n",
        "  text = re.sub(r\"\\bhttp://\\w+.\\w+/\\w+\",'',text)\n",
        "  text = re.sub(r\"\\@\\w+\",'',text)\n",
        "  text = re.sub(r\"\\#\\w+\",'',text)\n",
        "  text = re.sub(r'\\s+',' ',text) \n",
        "  text = re.sub(r'[^\\w\\s]','',text)\n",
        "  pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "  text = pattern.sub('', text)\n",
        "  text = corrector.FixFragment(text)\n",
        "  text=text.strip()\n",
        "  text_l= word_tokenize(text)\n",
        "  buff_str=''\n",
        "  for i in text_l:\n",
        "    if i in wordlist:\n",
        "      buff_str=buff_str+ i +' '\n",
        "  text=buff_str[:]\n",
        "  if text!=\"\":\n",
        "    processed_dataset = processed_dataset.append({'LABEL':d['LABEL'],'DATE_TIME':d['DATE_TIME'],'TEXT':text},ignore_index =True)\n",
        "processed_dataset.to_csv('processed_data.csv')\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6esIDm9Ki4_P"
      },
      "outputs": [],
      "source": [
        "dataset=pd.read_csv(\"processed_data.csv\")\n",
        "for i, row in dataset.iterrows():\n",
        "  tweets.append(row[\"TEXT\"])\n",
        "  if row[\"LABEL\"]==1:\n",
        "    pos.append(row[\"TEXT\"])\n",
        "  else:\n",
        "    neg.append(row[\"TEXT\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yi5E4JewUvJ"
      },
      "outputs": [],
      "source": [
        "unigram_dict={}\n",
        "bigram_dict={}\n",
        "\n",
        "for j in tweets:\n",
        "  buffarr=j.split()\n",
        "  for i in buffarr:\n",
        "    if i in unigram_dict:\n",
        "      unigram_dict[i]=unigram_dict[i]+1\n",
        "    else:\n",
        "      unigram_dict[i]=1\n",
        "  \n",
        "for i in tweets:\n",
        "  for j in i.split():\n",
        "    bigram_dict[j]={}\n",
        "\n",
        "for i in tweets:\n",
        "  buff_arr=i.split()\n",
        "  for j in range(1, len(buff_arr)):\n",
        "    if buff_arr[j-1] in bigram_dict[buff_arr[j]]:\n",
        "      bigram_dict[buff_arr[j]][buff_arr[j-1]]=bigram_dict[buff_arr[j]][buff_arr[j-1]] + 1\n",
        "    else:\n",
        "      bigram_dict[buff_arr[j]][buff_arr[j-1]]=1\n",
        "\n",
        "top4={}\n",
        "for i in bigram_dict:\n",
        "  top4[i]={}\n",
        "\n",
        "for i in bigram_dict:\n",
        "  for j in bigram_dict[i]:\n",
        "    top4[i][j]=bigram_dict[i][j]\n",
        "\n",
        "for i in bigram_dict:\n",
        "  for j in unigram_dict:\n",
        "    if j not in bigram_dict[i]:\n",
        "      bigram_dict[i][j]=0.5\n",
        "    else:\n",
        "      bigram_dict[i][j]+=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emV13fxBQ5iG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('convert.txt','w') as convert_file:\n",
        "  convert_file.write(json.dumps(bigram_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pngEsf2Bwzt-"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy5BmEtpr9yS"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "def sentiment_scores(sentence):\n",
        "    global sid_obj\n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "    if sentiment_dict['compound'] >= 0.05:\n",
        "        return [1, sentiment_dict['compound']]\n",
        "    elif sentiment_dict['compound'] < 0.05 and sentiment_dict['compound'] >= 0:\n",
        "        return [1, 0]\n",
        "    elif sentiment_dict['compound'] > -0.05 and sentiment_dict['compound'] < 0:\n",
        "      return [-1, 0]\n",
        "    else:\n",
        "      return [-1, -(sentiment_dict['compound'])]\n",
        "\n",
        "bigram_prob={}\n",
        "for i in bigram_dict:\n",
        "  bigram_prob[i]={}\n",
        "\n",
        "for i in bigram_dict:\n",
        "  for j in bigram_dict[i]:\n",
        "    bigram_prob[i][j]=bigram_dict[i][j]\n",
        "c=0\n",
        "v=len(unigram_dict)\n",
        "for word in bigram_prob:\n",
        "  # print(c)\n",
        "  c+=1\n",
        "  for prev in bigram_prob[word]:\n",
        "    string=word+' '+prev\n",
        "    sent=sentiment_scores(string)\n",
        "    bigram_prob[word][prev]=((bigram_dict[word][prev])/(unigram_dict[prev] + 0.5*v))*sent[0] + sent[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuxpRKomBR9z"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "keys_bigram = list(bigram_dict)\n",
        "sentences = []\n",
        "\n",
        "for i in range(250):\n",
        "  sen=\"\"\n",
        "  word = random.choice(keys_bigram)\n",
        "  sen = word[:]\n",
        "  arr=[word]\n",
        "  for j in range(10):\n",
        "    sub_dict = bigram_dict[word]\n",
        "    buff_dict={}\n",
        "    for i in sub_dict:\n",
        "      if i not in arr:\n",
        "        buff_dict[i]=sub_dict[i]\n",
        "    \n",
        "    maxprob=max(list(buff_dict.values()))\n",
        "    words=[i for i in buff_dict if buff_dict[i]==maxprob]\n",
        "    prev_word=random.choice(words)\n",
        "    arr.append(prev_word)\n",
        "    sen= prev_word+ ' ' + sen\n",
        "    word=prev_word[:]\n",
        "  sentences.append(sen)\n",
        "\n",
        "# for i in sentences:\n",
        "#   print(i)\n",
        "\n",
        "for i in range(250):\n",
        "  sen=\"\"\n",
        "  word = random.choice(keys_bigram)\n",
        "  sen = word[:]\n",
        "  arr=[word]\n",
        "  for j in range(10):\n",
        "    sub_dict = bigram_dict[word]\n",
        "    buff_dict={}\n",
        "    for i in sub_dict:\n",
        "      if i not in arr:\n",
        "        buff_dict[i]=sub_dict[i]\n",
        "    \n",
        "    minprob=min(list(buff_dict.values()))\n",
        "    words=[i for i in buff_dict if buff_dict[i]==minprob]\n",
        "    prev_word=random.choice(words)\n",
        "    arr.append(prev_word)\n",
        "    sen= prev_word+ ' ' + sen\n",
        "    word=prev_word[:]\n",
        "  sentences.append(sen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAog2SmfNIYZ"
      },
      "outputs": [],
      "source": [
        "def sentiment_scores1(sentence):\n",
        "    global sid_obj\n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "    if sentiment_dict['compound'] >= 0.05:\n",
        "        return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "generated_labels=[]\n",
        "for i in sentences:\n",
        "  label = sentiment_scores1(i)\n",
        "  if label==1:\n",
        "    generated_labels.append(0)\n",
        "  else:\n",
        "    generated_labels.append(1)\n",
        "generated_data = pd.DataFrame(list(zip(sentences, generated_labels)),\n",
        "               columns =['TEXT', 'LABEL'])\n",
        "generated_data.to_csv(\"generated_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi2XJaFAy09W"
      },
      "outputs": [],
      "source": [
        "def perplexity(sentence):\n",
        "  arr=sentence.split()\n",
        "  prob=1\n",
        "  for i in range(1, len(arr)):\n",
        "    prob=abs(bigram_prob[arr[i]][arr[i-1]])*prob\n",
        "\n",
        "  prob_inv=1/prob\n",
        "  power=1/len(arr)\n",
        "  perp=pow(prob_inv, power)\n",
        "\n",
        "  return perp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd3E1yMr_V4v"
      },
      "outputs": [],
      "source": [
        "sum=0\n",
        "# print(sentences)\n",
        "for i in sentences:\n",
        "  sum=sum+perplexity(i)\n",
        "\n",
        "avg=sum/500\n",
        "print(avg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_df = pd.DataFrame(columns=[\"PREV\",\"WORD\",\"COUNT\"])\n",
        "c=0\n",
        "for i in top4:\n",
        "  c+=1\n",
        "  # print(c)\n",
        "  for j in top4[i]:\n",
        "    bigram_df = bigram_df.append({\"PREV\":j,\"WORD\":i,\"COUNT\":bigram_dict[i][j]},ignore_index=True)\n",
        "bigram_df.nlargest(4, \"COUNT\")"
      ],
      "metadata": {
        "id": "WXtN17vCoojx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "print(\"POSITIVE SENTENCES\")\n",
        "for i,k in generated_data.iterrows():\n",
        "  if k['LABEL']==0 and count<=5:\n",
        "    print(k['TEXT'])\n",
        "    count +=1\n",
        "count=0\n",
        "print(\"NEGATIVE SENTENCES\")\n",
        "for i,k in generated_data.iterrows():\n",
        "  if k['LABEL']==1 and count<=5:\n",
        "    print(k['TEXT'])\n",
        "    count +=1"
      ],
      "metadata": {
        "id": "o7zPqiwfr1t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zFg696VTtS_"
      },
      "outputs": [],
      "source": [
        "dataA_B = pd.read_csv(\"A1_dataset.csv\")\n",
        "dataA_B = dataA_B.drop('DATE_TIME', axis=1)\n",
        "for i in range(len(sentences)):\n",
        "  dataA_B= dataA_B.append({\"TEXT\":sentences[i],\"LABEL\":generated_labels[i]},ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4XyY965Vn57"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "def train_and_evaluate(train_sentences, train_labels,\n",
        "  test_sentences, test_labels):\n",
        "  \"\"\"\n",
        "    parameters:\n",
        "    train_sentences : list of training sentences\n",
        "    train_labels : list of training labels\n",
        "    test_sentences : list of test sentences\n",
        "    test_labels : list of test labels\n",
        "    output:\n",
        "    accuracy : accuracy of the test set\n",
        "  \"\"\"\n",
        "  # Model building\n",
        "  model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "  # Training the model with the training data\n",
        "  model.fit(train_sentences, train_labels)\n",
        "  # Predicting the test data categories\n",
        "  predicted_test_labels = model.predict(test_sentences)\n",
        "  return accuracy_score(test_labels, predicted_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR3XaeBGWEtr"
      },
      "outputs": [],
      "source": [
        "DatasetA = pd.read_csv(\"A1_dataset.csv\")\n",
        "test_set = pd.read_csv(\"A2_test_dataset.csv\")\n",
        "train_sentences = DatasetA['TEXT'].tolist()\n",
        "train_labels = DatasetA['LABEL'].tolist()\n",
        "test_sentences = test_set['TEXT'].tolist()\n",
        "test_labels = test_set['LABEL'].tolist()\n",
        "acc_A = train_and_evaluate(train_sentences, train_labels,test_sentences, test_labels)\n",
        "print(acc_A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZaDKajXXqYI"
      },
      "outputs": [],
      "source": [
        "train_sentences = dataA_B['TEXT'].tolist()\n",
        "train_labels = dataA_B['LABEL'].tolist()\n",
        "acc_B = train_and_evaluate(train_sentences, train_labels,test_sentences, test_labels)\n",
        "print(acc_B)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}